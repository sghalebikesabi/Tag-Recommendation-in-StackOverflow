{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and classifying industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import datetime\n",
    "import scipy.integrate as integrate\n",
    "import math\n",
    "import numpy as np\n",
    "import operator as op\n",
    "import os\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sahra/Google Drive/LSE/ST442 - BML/Project'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading in the data, cleaning it and extracting the relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "chem = pd.read_excel(io=\"chemicals.xlsx\")\n",
    "soft = pd.read_excel(io=\"software.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PETRONAS CHEMICALS GP.</th>\n",
       "      <th>MITSUI CHEMICALS</th>\n",
       "      <th>TATA CHEMICALS</th>\n",
       "      <th>ABA CHEMICALS 'A'</th>\n",
       "      <th>BODAL CHEMICALS</th>\n",
       "      <th>CAPROLACTAM CHEMICALS</th>\n",
       "      <th>CHD CHEMICALS</th>\n",
       "      <th>CHEMBOND CHEMICALS</th>\n",
       "      <th>DYMATIC CHEMICALS 'A'</th>\n",
       "      <th>GARODIA CHEMICALS</th>\n",
       "      <th>...</th>\n",
       "      <th>JAPAN PURE CHEMICAL</th>\n",
       "      <th>JIANGSU FLAG CHEMICAL 'A'</th>\n",
       "      <th>JIANGSU YIDA CHEMICAL 'A'</th>\n",
       "      <th>JINAN ACETATE CHEMICAL</th>\n",
       "      <th>JIUTIAN CHEMICAL GP.</th>\n",
       "      <th>JORDAN CHEMICAL INDS. SUSP - SUSP.01/04/19</th>\n",
       "      <th>KAWAGUCHI CHEMICAL IND.</th>\n",
       "      <th>KG CHEMICAL</th>\n",
       "      <th>KO YO CHEMICAL (GROUP)</th>\n",
       "      <th>KODAMA CHEMICAL IND.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>81.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.520123</td>\n",
       "      <td>1621.515152</td>\n",
       "      <td>368.880808</td>\n",
       "      <td>7.588451</td>\n",
       "      <td>40.817778</td>\n",
       "      <td>10.256768</td>\n",
       "      <td>9.601250</td>\n",
       "      <td>119.068990</td>\n",
       "      <td>8.673131</td>\n",
       "      <td>39.886912</td>\n",
       "      <td>...</td>\n",
       "      <td>2348.616162</td>\n",
       "      <td>57.516250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126.038095</td>\n",
       "      <td>0.065192</td>\n",
       "      <td>1.935253</td>\n",
       "      <td>1203.232323</td>\n",
       "      <td>13122.929293</td>\n",
       "      <td>0.692495</td>\n",
       "      <td>695.858586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.504367</td>\n",
       "      <td>564.724267</td>\n",
       "      <td>94.343055</td>\n",
       "      <td>3.310733</td>\n",
       "      <td>43.587939</td>\n",
       "      <td>9.817166</td>\n",
       "      <td>1.911645</td>\n",
       "      <td>62.443197</td>\n",
       "      <td>2.890721</td>\n",
       "      <td>7.693008</td>\n",
       "      <td>...</td>\n",
       "      <td>363.749233</td>\n",
       "      <td>3.798063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.306144</td>\n",
       "      <td>0.048240</td>\n",
       "      <td>0.515697</td>\n",
       "      <td>279.211091</td>\n",
       "      <td>3953.066558</td>\n",
       "      <td>0.432968</td>\n",
       "      <td>163.513777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.130000</td>\n",
       "      <td>765.000000</td>\n",
       "      <td>216.350000</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.040000</td>\n",
       "      <td>7.780000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>4.420000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1807.000000</td>\n",
       "      <td>51.380000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.300000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.320000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>6200.000000</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>430.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.240000</td>\n",
       "      <td>1237.500000</td>\n",
       "      <td>312.750000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>10.375000</td>\n",
       "      <td>3.040000</td>\n",
       "      <td>8.260000</td>\n",
       "      <td>75.750000</td>\n",
       "      <td>5.875000</td>\n",
       "      <td>31.950000</td>\n",
       "      <td>...</td>\n",
       "      <td>2107.500000</td>\n",
       "      <td>55.820000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.500000</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>1.520000</td>\n",
       "      <td>1010.000000</td>\n",
       "      <td>9655.000000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>555.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.610000</td>\n",
       "      <td>1405.000000</td>\n",
       "      <td>342.350000</td>\n",
       "      <td>7.730000</td>\n",
       "      <td>29.800000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>8.720000</td>\n",
       "      <td>91.150000</td>\n",
       "      <td>8.580000</td>\n",
       "      <td>43.350000</td>\n",
       "      <td>...</td>\n",
       "      <td>2305.000000</td>\n",
       "      <td>56.540000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1110.000000</td>\n",
       "      <td>13750.000000</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>412.625000</td>\n",
       "      <td>10.020000</td>\n",
       "      <td>41.350000</td>\n",
       "      <td>13.875000</td>\n",
       "      <td>10.330000</td>\n",
       "      <td>181.675000</td>\n",
       "      <td>10.950000</td>\n",
       "      <td>46.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>2497.500000</td>\n",
       "      <td>60.525000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>2.240000</td>\n",
       "      <td>1385.000000</td>\n",
       "      <td>16275.000000</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>820.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.620000</td>\n",
       "      <td>3110.000000</td>\n",
       "      <td>656.800000</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>179.850000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>48.490000</td>\n",
       "      <td>...</td>\n",
       "      <td>3800.000000</td>\n",
       "      <td>62.550000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>3.040000</td>\n",
       "      <td>2220.000000</td>\n",
       "      <td>23450.000000</td>\n",
       "      <td>2.660000</td>\n",
       "      <td>1080.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PETRONAS CHEMICALS GP.  MITSUI CHEMICALS  TATA CHEMICALS  \\\n",
       "count               81.000000         99.000000       99.000000   \n",
       "mean                 6.520123       1621.515152      368.880808   \n",
       "std                  0.504367        564.724267       94.343055   \n",
       "min                  5.130000        765.000000      216.350000   \n",
       "25%                  6.240000       1237.500000      312.750000   \n",
       "50%                  6.610000       1405.000000      342.350000   \n",
       "75%                  6.800000       1885.000000      412.625000   \n",
       "max                  7.620000       3110.000000      656.800000   \n",
       "\n",
       "       ABA CHEMICALS 'A'  BODAL CHEMICALS  CAPROLACTAM CHEMICALS  \\\n",
       "count          71.000000        99.000000              99.000000   \n",
       "mean            7.588451        40.817778              10.256768   \n",
       "std             3.310733        43.587939               9.817166   \n",
       "min             2.650000         6.000000               3.040000   \n",
       "25%             4.630000        10.375000               3.040000   \n",
       "50%             7.730000        29.800000               6.300000   \n",
       "75%            10.020000        41.350000              13.875000   \n",
       "max            17.650000       179.850000              50.000000   \n",
       "\n",
       "       CHD CHEMICALS  CHEMBOND CHEMICALS  DYMATIC CHEMICALS 'A'  \\\n",
       "count      16.000000           99.000000              99.000000   \n",
       "mean        9.601250          119.068990               8.673131   \n",
       "std         1.911645           62.443197               2.890721   \n",
       "min         7.780000           32.000000               4.420000   \n",
       "25%         8.260000           75.750000               5.875000   \n",
       "50%         8.720000           91.150000               8.580000   \n",
       "75%        10.330000          181.675000              10.950000   \n",
       "max        14.000000          251.000000              15.800000   \n",
       "\n",
       "       GARODIA CHEMICALS          ...           JAPAN PURE CHEMICAL  \\\n",
       "count          68.000000          ...                     99.000000   \n",
       "mean           39.886912          ...                   2348.616162   \n",
       "std             7.693008          ...                    363.749233   \n",
       "min            27.000000          ...                   1807.000000   \n",
       "25%            31.950000          ...                   2107.500000   \n",
       "50%            43.350000          ...                   2305.000000   \n",
       "75%            46.100000          ...                   2497.500000   \n",
       "max            48.490000          ...                   3800.000000   \n",
       "\n",
       "       JIANGSU FLAG CHEMICAL 'A'  JIANGSU YIDA CHEMICAL 'A'  \\\n",
       "count                   8.000000                        0.0   \n",
       "mean                   57.516250                        NaN   \n",
       "std                     3.798063                        NaN   \n",
       "min                    51.380000                        NaN   \n",
       "25%                    55.820000                        NaN   \n",
       "50%                    56.540000                        NaN   \n",
       "75%                    60.525000                        NaN   \n",
       "max                    62.550000                        NaN   \n",
       "\n",
       "       JINAN ACETATE CHEMICAL  JIUTIAN CHEMICAL GP.  \\\n",
       "count               21.000000             99.000000   \n",
       "mean               126.038095              0.065192   \n",
       "std                 27.306144              0.048240   \n",
       "min                 88.300000              0.012000   \n",
       "25%                107.500000              0.033500   \n",
       "50%                116.000000              0.054000   \n",
       "75%                144.000000              0.082000   \n",
       "max                181.000000              0.260000   \n",
       "\n",
       "       JORDAN CHEMICAL INDS. SUSP - SUSP.01/04/19  KAWAGUCHI CHEMICAL IND.  \\\n",
       "count                                   99.000000                99.000000   \n",
       "mean                                     1.935253              1203.232323   \n",
       "std                                      0.515697               279.211091   \n",
       "min                                      1.320000               740.000000   \n",
       "25%                                      1.520000              1010.000000   \n",
       "50%                                      1.750000              1110.000000   \n",
       "75%                                      2.240000              1385.000000   \n",
       "max                                      3.040000              2220.000000   \n",
       "\n",
       "        KG CHEMICAL  KO YO CHEMICAL (GROUP)  KODAMA CHEMICAL IND.  \n",
       "count     99.000000               99.000000             99.000000  \n",
       "mean   13122.929293                0.692495            695.858586  \n",
       "std     3953.066558                0.432968            163.513777  \n",
       "min     6200.000000                0.111000            430.000000  \n",
       "25%     9655.000000                0.410000            555.000000  \n",
       "50%    13750.000000                0.605000            660.000000  \n",
       "75%    16275.000000                0.845000            820.000000  \n",
       "max    23450.000000                2.660000           1080.000000  \n",
       "\n",
       "[8 rows x 117 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chem[1:100].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONSTELLATION SOFTWARE</th>\n",
       "      <th>TABLEAU SOFTWARE CL.A</th>\n",
       "      <th>GUIDEWIRE SOFTWARE</th>\n",
       "      <th>PAYCOM SOFTWARE</th>\n",
       "      <th>ULTIMATE SOFTWARE GP.</th>\n",
       "      <th>COUPA SOFTWARE</th>\n",
       "      <th>CYBER ARK SOFTWARE</th>\n",
       "      <th>DHC SOFTWARE 'A'</th>\n",
       "      <th>SOFTWARE N</th>\n",
       "      <th>ALFA FINANCIAL SOFTWARE HOLDINGS(WI)</th>\n",
       "      <th>...</th>\n",
       "      <th>FLEXI INTL.SFTW.</th>\n",
       "      <th>FUJIAN RONGJI SFTW.'A'</th>\n",
       "      <th>INDO-PAC.SFTW.&amp; ENTM.</th>\n",
       "      <th>INNVN.SFTW.EXPS.</th>\n",
       "      <th>JIANGSU HOPERUN SFTW.'A'</th>\n",
       "      <th>RENOWORKS SFTW.</th>\n",
       "      <th>BEIJING JOIN-CHEER SFTW. 'A'</th>\n",
       "      <th>CYBERTECH SYS.&amp; SFTW.</th>\n",
       "      <th>INDIAN INFOTECH &amp; SFTW.</th>\n",
       "      <th>SICHUAN JIUYUAN YINHAI SFTW.'A'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>256.099495</td>\n",
       "      <td>69.885294</td>\n",
       "      <td>47.294925</td>\n",
       "      <td>37.820000</td>\n",
       "      <td>116.200657</td>\n",
       "      <td>28.295000</td>\n",
       "      <td>48.020857</td>\n",
       "      <td>7.384343</td>\n",
       "      <td>28.597919</td>\n",
       "      <td>455.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200928</td>\n",
       "      <td>9.745060</td>\n",
       "      <td>1.891111</td>\n",
       "      <td>2.748586</td>\n",
       "      <td>11.773443</td>\n",
       "      <td>0.147778</td>\n",
       "      <td>7.904792</td>\n",
       "      <td>29.754040</td>\n",
       "      <td>7.598941</td>\n",
       "      <td>44.246842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>222.658288</td>\n",
       "      <td>19.549266</td>\n",
       "      <td>12.225728</td>\n",
       "      <td>15.129718</td>\n",
       "      <td>64.061728</td>\n",
       "      <td>3.330757</td>\n",
       "      <td>8.378891</td>\n",
       "      <td>3.991865</td>\n",
       "      <td>5.920462</td>\n",
       "      <td>27.086666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129971</td>\n",
       "      <td>4.802298</td>\n",
       "      <td>1.307209</td>\n",
       "      <td>2.131873</td>\n",
       "      <td>5.949346</td>\n",
       "      <td>0.150710</td>\n",
       "      <td>5.528814</td>\n",
       "      <td>21.596027</td>\n",
       "      <td>11.293855</td>\n",
       "      <td>8.932433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>32.600000</td>\n",
       "      <td>41.680000</td>\n",
       "      <td>17.830000</td>\n",
       "      <td>12.880000</td>\n",
       "      <td>20.310000</td>\n",
       "      <td>24.570000</td>\n",
       "      <td>27.390000</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>16.373000</td>\n",
       "      <td>429.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>33.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>69.450000</td>\n",
       "      <td>53.580000</td>\n",
       "      <td>39.220000</td>\n",
       "      <td>29.800000</td>\n",
       "      <td>54.065000</td>\n",
       "      <td>25.672500</td>\n",
       "      <td>42.890000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>24.931500</td>\n",
       "      <td>441.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>6.140000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>6.590000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>13.425000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>35.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>145.410000</td>\n",
       "      <td>65.490000</td>\n",
       "      <td>49.330000</td>\n",
       "      <td>36.615000</td>\n",
       "      <td>117.290000</td>\n",
       "      <td>27.100000</td>\n",
       "      <td>48.350000</td>\n",
       "      <td>5.450000</td>\n",
       "      <td>27.971000</td>\n",
       "      <td>453.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>7.510000</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>1.890000</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>5.270000</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>43.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>503.180000</td>\n",
       "      <td>81.110000</td>\n",
       "      <td>56.215000</td>\n",
       "      <td>47.710000</td>\n",
       "      <td>170.790000</td>\n",
       "      <td>30.557500</td>\n",
       "      <td>52.305000</td>\n",
       "      <td>10.505000</td>\n",
       "      <td>32.499000</td>\n",
       "      <td>468.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>3.965000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>12.670000</td>\n",
       "      <td>43.850000</td>\n",
       "      <td>13.630000</td>\n",
       "      <td>50.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>693.400000</td>\n",
       "      <td>113.810000</td>\n",
       "      <td>71.920000</td>\n",
       "      <td>70.460000</td>\n",
       "      <td>227.210000</td>\n",
       "      <td>34.620000</td>\n",
       "      <td>66.500000</td>\n",
       "      <td>21.470000</td>\n",
       "      <td>42.507000</td>\n",
       "      <td>483.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>26.020000</td>\n",
       "      <td>5.850000</td>\n",
       "      <td>9.610000</td>\n",
       "      <td>31.830000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>28.300000</td>\n",
       "      <td>92.700000</td>\n",
       "      <td>44.750000</td>\n",
       "      <td>65.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CONSTELLATION SOFTWARE  TABLEAU SOFTWARE CL.A  GUIDEWIRE SOFTWARE  \\\n",
       "count               99.000000              51.000000           67.000000   \n",
       "mean               256.099495              69.885294           47.294925   \n",
       "std                222.658288              19.549266           12.225728   \n",
       "min                 32.600000              41.680000           17.830000   \n",
       "25%                 69.450000              53.580000           39.220000   \n",
       "50%                145.410000              65.490000           49.330000   \n",
       "75%                503.180000              81.110000           56.215000   \n",
       "max                693.400000             113.810000           71.920000   \n",
       "\n",
       "       PAYCOM SOFTWARE  ULTIMATE SOFTWARE GP.  COUPA SOFTWARE  \\\n",
       "count        40.000000              99.000000       10.000000   \n",
       "mean         37.820000             116.200657       28.295000   \n",
       "std          15.129718              64.061728        3.330757   \n",
       "min          12.880000              20.310000       24.570000   \n",
       "25%          29.800000              54.065000       25.672500   \n",
       "50%          36.615000             117.290000       27.100000   \n",
       "75%          47.710000             170.790000       30.557500   \n",
       "max          70.460000             227.210000       34.620000   \n",
       "\n",
       "       CYBER ARK SOFTWARE  DHC SOFTWARE 'A'  SOFTWARE N  \\\n",
       "count           35.000000         99.000000   99.000000   \n",
       "mean            48.020857          7.384343   28.597919   \n",
       "std              8.378891          3.991865    5.920462   \n",
       "min             27.390000          2.330000   16.373000   \n",
       "25%             42.890000          4.000000   24.931500   \n",
       "50%             48.350000          5.450000   27.971000   \n",
       "75%             52.305000         10.505000   32.499000   \n",
       "max             66.500000         21.470000   42.507000   \n",
       "\n",
       "       ALFA FINANCIAL SOFTWARE HOLDINGS(WI)               ...                 \\\n",
       "count                              3.000000               ...                  \n",
       "mean                             455.500000               ...                  \n",
       "std                               27.086666               ...                  \n",
       "min                              429.750000               ...                  \n",
       "25%                              441.375000               ...                  \n",
       "50%                              453.000000               ...                  \n",
       "75%                              468.375000               ...                  \n",
       "max                              483.750000               ...                  \n",
       "\n",
       "       FLEXI INTL.SFTW.  FUJIAN RONGJI SFTW.'A'  INDO-PAC.SFTW.& ENTM.  \\\n",
       "count         99.000000               83.000000              99.000000   \n",
       "mean           0.200928                9.745060               1.891111   \n",
       "std            0.129971                4.802298               1.307209   \n",
       "min            0.040000                4.020000               0.480000   \n",
       "25%            0.110000                6.140000               0.940000   \n",
       "50%            0.160000                7.510000               1.560000   \n",
       "75%            0.222500               12.700000               2.170000   \n",
       "max            0.550000               26.020000               5.850000   \n",
       "\n",
       "       INNVN.SFTW.EXPS.  JIANGSU HOPERUN SFTW.'A'  RENOWORKS SFTW.  \\\n",
       "count         99.000000                 61.000000        99.000000   \n",
       "mean           2.748586                 11.773443         0.147778   \n",
       "std            2.131873                  5.949346         0.150710   \n",
       "min            0.550000                  3.470000         0.010000   \n",
       "25%            0.950000                  6.590000         0.030000   \n",
       "50%            1.890000                 11.600000         0.050000   \n",
       "75%            3.965000                 15.500000         0.290000   \n",
       "max            9.610000                 31.830000         0.570000   \n",
       "\n",
       "       BEIJING JOIN-CHEER SFTW. 'A'  CYBERTECH SYS.& SFTW.  \\\n",
       "count                     96.000000              99.000000   \n",
       "mean                       7.904792              29.754040   \n",
       "std                        5.528814              21.596027   \n",
       "min                        2.130000               9.900000   \n",
       "25%                        3.760000              13.425000   \n",
       "50%                        5.270000              17.950000   \n",
       "75%                       12.670000              43.850000   \n",
       "max                       28.300000              92.700000   \n",
       "\n",
       "       INDIAN INFOTECH & SFTW.  SICHUAN JIUYUAN YINHAI SFTW.'A'  \n",
       "count                85.000000                        19.000000  \n",
       "mean                  7.598941                        44.246842  \n",
       "std                  11.293855                         8.932433  \n",
       "min                   0.160000                        33.110000  \n",
       "25%                   0.270000                        35.835000  \n",
       "50%                   1.660000                        43.440000  \n",
       "75%                  13.630000                        50.650000  \n",
       "max                  44.750000                        65.750000  \n",
       "\n",
       "[8 rows x 120 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft[1:100].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with at least 1% missing values\n",
    "chem = chem[chem.columns[chem.isnull().mean() < 0.01]]\n",
    "soft = soft[soft.columns[soft.isnull().mean() < 0.01]]\n",
    "\n",
    "# drop columns with zero values\n",
    "chem = chem[chem>0]\n",
    "soft = soft[soft>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices(data, frequency, daily_data = False):\n",
    "    \"\"\" Gives indices of days that singalize the beginning of new period\n",
    "    Args: \n",
    "        daily_data - data set with datetime64-type column Name\n",
    "        frequency - monthly, quarterly or annual\n",
    "    Returns: \n",
    "        indices - list of vector indices that are the first day in a \n",
    "        month, quarter or year \n",
    "    \"\"\"\n",
    "    dates = data[\"Name\"]\n",
    "    months = np.zeros(len(dates))\n",
    "    years = np.zeros(len(dates))\n",
    "    for i in range(len(dates)):\n",
    "        months[i] = dates[i].month\n",
    "        years[i] = dates[i].year\n",
    "    old_month = 0\n",
    "    old_year = 0\n",
    "    indices = [0]\n",
    "    \n",
    "    for i in range(len(years)):\n",
    "        if frequency == \"annually\" or frequency == \"biannually\":\n",
    "            current_year = years[i]\n",
    "            if current_year != years[old_year]:\n",
    "                indices.append(i)\n",
    "                old_year = i\n",
    "                \n",
    "    else:\n",
    "        for i in range(len(months)):\n",
    "            current_month = months[i]\n",
    "            if current_month != months[old_month]:\n",
    "                old_month = i\n",
    "                if frequency==\"monthly\":\n",
    "                    indices.append(i)\n",
    "                elif frequency==\"quarterly\":\n",
    "                    if (current_month==1 or current_month==4 or \n",
    "                        current_month==7 or current_month==10):\n",
    "                        indices.append(i)\n",
    "    \n",
    "    if frequency == \"biannually\":\n",
    "            indices = indices[::2]\n",
    "        \n",
    "    return(indices)\n",
    "\n",
    "\n",
    "def returns(daily_data, frequency, drop = True, vol = False):\n",
    "    \"\"\" Extracts attributes from daily data  \n",
    "        Args: \n",
    "            daily_data - daily data as pandas dataframe\n",
    "            frequency - daily, monthly, quarterly, annual\n",
    "            drop - if 'True', drop companies with more than 10% NaN values\n",
    "            vol - if 'True', computes standard deviation instead of mean \n",
    "         Returns: \n",
    "            return - monthly, quarterly or annual return resp. average data in pandas Dataframe\n",
    "    \"\"\"\n",
    "    daily_data_df = daily_data\n",
    "    if frequency != \"daily\":\n",
    "        indices_data = indices(daily_data, frequency)\n",
    "        daily_data = daily_data.values\n",
    "        returns = np.zeros((len(indices_data)-1,daily_data.shape[1]-1))\n",
    "        for i in range(len(indices_data)-1):\n",
    "            if vol==False:\n",
    "                returns[i,:] = np.divide(daily_data[indices_data[i+1],1:], \n",
    "                                          daily_data[indices_data[i],1:]) - 1\n",
    "            else:\n",
    "                returns[i,:] = [np.std(daily_data[range(indices_data[i],indices_data[i+1]),j]) for j in range(1,daily_data.shape[1])]\n",
    "        if drop==True:\n",
    "            # delete columns if more than 10% of the returns are zero\n",
    "            returns_zero = [np.mean(returns[:,i]==0) \n",
    "                            for i in range(returns.shape[1])]\n",
    "\n",
    "            returns = returns[:,[returns_zero[i]<0.1 \n",
    "                                 for i in range(len(returns_zero))]]\n",
    "\n",
    "            dropped = [returns_zero[i]<0.1 for i in range(len(returns_zero))]\n",
    "            dropped = [i*(dropped[i]==True) for i in range(len(returns_zero))]\n",
    "            dropped = np.transpose(dropped)\n",
    "            dropped = dropped[dropped!=0]\n",
    "            return(pd.DataFrame(returns, columns=daily_data_df.columns.values[dropped]))\n",
    "\n",
    "        else:\n",
    "            return(pd.DataFrame(returns, columns=daily_data_df.columns.values[1:]))\n",
    "    else:\n",
    "        daily_data = daily_data.values\n",
    "        return([np.std(daily_data[:,j]) for j in range(1,daily_data.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As features we regard the quarterly and annual returns and annual volatility of the period regarded. We save these measures in the arrays 'merged_chem' and 'merged_tech'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge quarterly and annual returns (in  numpy array) for chemical and technical industry\n",
    "merged_chem = np.row_stack((returns(chem,\"annually\",drop=False),\n",
    "                            returns(chem,\"annually\",drop=False,vol=True)))\n",
    "merged_soft = np.row_stack((returns(soft,\"annually\",drop=False),\n",
    "                            returns(soft,\"annually\",drop=False,vol=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete columns if any attribute is NA\n",
    "zero_clms = [np.mean(merged_chem[:,i]==0) \n",
    "                for i in range(merged_chem.shape[1])] \n",
    "merged_chem = merged_chem[:,[zero_clms[i]<0.1 \n",
    "                     for i in range(len(zero_clms))]]\n",
    "\n",
    "zero_clms = [np.mean(merged_soft[:,i]==0) \n",
    "                for i in range(merged_soft.shape[1])] \n",
    "merged_soft = merged_soft[:,[zero_clms[i]<0.1 \n",
    "                     for i in range(len(zero_clms))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only analyse the 100 best performing stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge merged_chem and merged_tech\n",
    "merged_data = np.column_stack((merged_chem, merged_soft))\n",
    "merged_classes = np.concatenate((np.zeros(merged_chem.shape[1]), \n",
    "                                 np.zeros(merged_soft.shape[1])+1))\n",
    "\n",
    "# delete columns with too many zero returns\n",
    "merged_zero = [np.mean(merged_data[:,i]==0) \n",
    "               for i in range(merged_data.shape[1])]\n",
    "\n",
    "merged_classes = merged_classes[[merged_zero[i]<0.1 \n",
    "                                 for i in range(merged_data.shape[1])]]\n",
    "\n",
    "merged_data = merged_data[:,[merged_zero[i]<0.1 \n",
    "                             for i in range(merged_data.shape[1])]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 152)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge merged_chem and merged_tech\n",
    "merged_data = np.column_stack((merged_chem, merged_soft))\n",
    "merged_classes = np.concatenate((np.zeros(merged_chem.shape[1]), \n",
    "                                 np.zeros(merged_soft.shape[1])+1))\n",
    "# delete columns with too many zero returns\n",
    "merged_zero = [np.mean(merged_data[:,i]==0) \n",
    "               for i in range(merged_data.shape[1])]\n",
    "\n",
    "merged_classes = merged_classes[[merged_zero[i]<0.1 \n",
    "                                 for i in range(merged_data.shape[1])]]\n",
    "\n",
    "merged_data = merged_data[:,[merged_zero[i]<0.1 \n",
    "                             for i in range(merged_data.shape[1])]] \n",
    "\n",
    "# 120 stocks with best performance in period under consideration\n",
    "take_in = returns(chem,\"annually\",drop=False).shape[0]\n",
    "total_merged = np.array([np.prod(1+merged_data[range(take_in),i]) \n",
    "                         for i in range(merged_data.shape[1])])  \n",
    "\n",
    "top120 = total_merged.argsort()[-120:][::-1]\n",
    "top120_data = np.transpose(merged_data[:,top120])\n",
    "top120_classes = merged_classes[top120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we analyse the data further, we normalize the features by substracting the mean and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_it(data):\n",
    "    \"\"\"Standardizes input\"\"\"\n",
    "    mean = [np.mean(data[:,i]) for i in range(data.shape[1])]\n",
    "    std = [np.std(data[:,i]) for i in range(data.shape[1])]\n",
    "    normed = np.array([[(data[i,j] - mean[j])/std[j] for j in range(data.shape[1])] for i in range(data.shape[0])])\n",
    "    return(normed)\n",
    "\n",
    "top120_data = norm_it(top120_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(451)\n",
    "\n",
    "# extract train and test data according to 70:50 split\n",
    "train_indices = random.sample(range(120),100)\n",
    "test_indices = list(set(range(120)) - set(train_indices))\n",
    "y_train, X_train = top120_classes[train_indices], top120_data[train_indices,:]\n",
    "y_test, X_test = top120_classes[test_indices], top120_data[test_indices,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi collinearity  analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.9014,\n",
       " 2.0474,\n",
       " 1.4839,\n",
       " 1.4332,\n",
       " 1.1012,\n",
       " 0.875,\n",
       " 0.839,\n",
       " 0.7077,\n",
       " 0.6003,\n",
       " 0.4299,\n",
       " 0.3436,\n",
       " 0.1457,\n",
       " 0.062,\n",
       " 0.0121,\n",
       " 0.0081,\n",
       " 0.0048,\n",
       " 0.0035,\n",
       " 0.0011,\n",
       " 0.0002,\n",
       " 0.0001]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute eigenvalues in order to avoid multi-collinearity\n",
    "p = X_train.shape[1]\n",
    "p_new = p\n",
    "corr = np.corrcoef(X_train, rowvar=0)\n",
    "w, v = np.linalg.eig(corr)       \n",
    "[np.round(w[i],4) for i in range(p_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =  X_train[:,:p_new]\n",
    "X_test = X_test[:,:p_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We consider 120 stocks with 20 features during the period from 2009-04-29 to 2019-04-29.\n"
     ]
    }
   ],
   "source": [
    "print(\"We consider {} stocks with {} features during the period from {} to {}.\".format(top120_data.shape[0], X_train.shape[1], chem['Name'][0].date(), chem['Name'][chem.shape[0]-1].date()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among the 120 best performing stocks of the past two years we have 55.0 stocks from the software sector.\n"
     ]
    }
   ],
   "source": [
    "print(\"Among the 120 best performing stocks of the past two years we have {} stocks from the software sector.\".format(np.sum(top120_classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the classification of the selected stocks $x_1,...,x_n$  with $n=100$ for the event $Y=1$, i.e. the stock belongs to technology industry, follows a binomial distribution with parameter $p$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(top120_classes)\n",
    "p = X_train.shape[1]\n",
    "y = np.sum(top120_classes)\n",
    "\n",
    "# frequentist descriptive statistics\n",
    "\n",
    "p_mean = y/n\n",
    "p_mode = y/n\n",
    "p_median = y/n\n",
    "p_std = p_mean*(1-p_mean)/n\n",
    "p_conf095 = [p_mean/n - 1.96*p_std,p_mean/n + 1.96*p_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian descriptive statistics\n",
    "\n",
    "def nck(n, k):\n",
    "    \"\"\"n chooses k\"\"\"\n",
    "    n = int(n)\n",
    "    k = int(k)\n",
    "    r = min(k, n-k)\n",
    "    num = reduce(op.mul, range(n, n-r, -1), 1)\n",
    "    den = reduce(op.mul, range(1, r+1), 1)\n",
    "    return num / den\n",
    "\n",
    "def posterior(theta):\n",
    "    \"\"\"Computes the posterior pi(theta|y) given y is binomially distributed\n",
    "    and theta uniformly distributed on [0.5,1] up to proportionality\"\"\"\n",
    "    if (theta > 0.5) and (theta < 1):\n",
    "        return(theta**y*(1-theta)**(n-y))\n",
    "    else: \n",
    "        return(0)\n",
    "\n",
    "def help_exp(theta):\n",
    "    \"\"\"pi(theta|y)*theta given y is binomially distributed and theta \n",
    "    uniformly distributed on [0.5,1] up to proportionality\"\"\"\n",
    "    if (theta > 0.5) and (theta < 1):\n",
    "        return(theta**(y+1)*(1-theta)**(n-y))\n",
    "    else: \n",
    "        return(0)\n",
    "\n",
    "# posterior mean with integral\n",
    "integral_posterior = integrate.quad(posterior,0.5,1)[0]\n",
    "save = integrate.quad(help_exp,0.5,1)[0]\n",
    "post_mean = save/integral_posterior\n",
    "\n",
    "# posterior mean and median based on Monte Carlo\n",
    "size = 1000000\n",
    "sample = np.random.beta(a=y+1,b=n-y+1,size=size)\n",
    "sample = sample[[sample[i]>0.5 for i in range(size)]]\n",
    "post_mean = np.mean(sample)\n",
    "post_median = np.median(sample)\n",
    "credint095 = [np.quantile(sample,0.025),np.quantile(sample,0.975)]\n",
    "# posterior mode based on optimization\n",
    "thetas = [i/2000+0.5 for i in range(1000)]\n",
    "posteriors = [posterior(thetas[i]) for i in range(1000)]\n",
    "post_mode = thetas[np.argmax(posteriors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_df = pd.DataFrame(data={'Frequentist estimators':\n",
    "  [p_mean,p_mode,p_median,p_conf095[0],p_conf095[1]],'Bayesian estimators':\n",
    "  [post_mean,post_mode,post_median,credint095[0],credint095[1]]},\n",
    "  index=['mean', 'mode', 'median', 'lower 5% bound', 'upper 5% bound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "{} &  Frequentist estimators &  Bayesian estimators \\\\\n",
      "\\midrule\n",
      "mean           &                0.458333 &             0.524557 \\\\\n",
      "mode           &                0.458333 &             0.500500 \\\\\n",
      "median         &                0.458333 &             0.519343 \\\\\n",
      "lower 5\\% bound &               -0.000236 &             0.500777 \\\\\n",
      "upper 5\\% bound &                0.007874 &             0.576749 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(estimates_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.06338019, -0.02680527,  0.00580608, -0.11511457, -0.13036258,\n",
       "         -0.04689206, -0.14040496,  0.18364797, -0.08808063, -0.18765357,\n",
       "          0.21588743,  0.17272398,  0.19378715,  0.17191618,  0.18826972,\n",
       "          0.20893629,  0.22136161,  0.19662697,  0.2206557 ,  0.16382194],\n",
       "        [-0.0014257 , -0.01820816, -0.03233294,  0.17122438,  0.21197888,\n",
       "          0.04299077,  0.18434816, -0.23885735,  0.14331788,  0.10018602,\n",
       "         -0.18850588, -0.14338712, -0.16958257, -0.15237958, -0.15940688,\n",
       "         -0.17898903, -0.18935873, -0.1853969 , -0.1935005 , -0.13752094]]),\n",
       " array([0.54, 0.46]),\n",
       " array([[ 1.00961554, -0.21826781, -0.03990405, -0.02719648, -0.0536796 ,\n",
       "          0.01865632, -0.08148403,  0.13981409,  0.32071437, -0.42655374,\n",
       "         -0.07849852, -0.0625894 , -0.10638233, -0.06399604, -0.06722267,\n",
       "         -0.06021107, -0.07283895, -0.0572483 , -0.07002818, -0.02651736],\n",
       "        [-0.21826781,  1.03474369, -0.10393618,  0.26884933, -0.15707337,\n",
       "          0.30155532, -0.0977146 , -0.03877177, -0.11626891,  0.01946647,\n",
       "          0.54515472,  0.5254915 ,  0.65648999,  0.48385485,  0.44037878,\n",
       "          0.44438449,  0.46293975,  0.3993541 ,  0.51153128,  0.34199703],\n",
       "        [-0.03990405, -0.10393618,  1.04366718, -0.09901942, -0.10410512,\n",
       "         -0.18396486,  0.03899103, -0.2318474 ,  0.29161462,  0.01772517,\n",
       "          0.02864686,  0.01319037,  0.10259754,  0.10398167,  0.15931258,\n",
       "          0.07760908,  0.11306478,  0.11240146,  0.04528774,  0.03214024],\n",
       "        [-0.02719648,  0.26884933, -0.09901942,  1.13131411, -0.09939988,\n",
       "          0.68566544, -0.17618028,  0.06670513, -0.1377392 ,  0.15672108,\n",
       "         -0.07759887, -0.08396534, -0.08410067, -0.07247051, -0.06420984,\n",
       "         -0.06823602, -0.05809714, -0.05511243, -0.08118742, -0.07043258],\n",
       "        [-0.0536796 , -0.15707337, -0.10410512, -0.09939988,  1.01747152,\n",
       "         -0.13901268, -0.17478549,  0.0337226 , -0.08449127, -0.02454069,\n",
       "         -0.12747447, -0.12205171, -0.14503987, -0.12625888, -0.11951228,\n",
       "         -0.11724608, -0.12193292, -0.11291085, -0.11902332, -0.09996177],\n",
       "        [ 0.01865632,  0.30155532, -0.18396486,  0.68566544, -0.13901268,\n",
       "          1.12283238, -0.06000479, -0.05935486, -0.09011994,  0.23357364,\n",
       "         -0.09361672, -0.08394645, -0.09491994, -0.08410296, -0.09002395,\n",
       "         -0.07850486, -0.06423026, -0.06081991, -0.09106937, -0.07574246],\n",
       "        [-0.08148403, -0.0977146 ,  0.03899103, -0.17618028, -0.17478549,\n",
       "         -0.06000479,  1.11616802, -0.04324265, -0.0824792 ,  0.0088998 ,\n",
       "          0.0719206 ,  0.09733441,  0.0846838 ,  0.11169439,  0.12136506,\n",
       "          0.12538785,  0.1137582 ,  0.11056804,  0.1053126 ,  0.13415453],\n",
       "        [ 0.13981409, -0.03877177, -0.2318474 ,  0.06670513,  0.0337226 ,\n",
       "         -0.05935486, -0.04324265,  1.06333327, -0.21956714, -0.09068006,\n",
       "         -0.01474264,  0.00247296, -0.00958805, -0.01344733, -0.03076694,\n",
       "         -0.02994661, -0.03623643, -0.0260855 , -0.02150154, -0.01481751],\n",
       "        [ 0.32071437, -0.11626891,  0.29161462, -0.1377392 , -0.08449127,\n",
       "         -0.09011994, -0.0824792 , -0.21956714,  1.18462568, -0.1466825 ,\n",
       "         -0.00402781, -0.00224139, -0.00250723, -0.00263364, -0.00297991,\n",
       "         -0.00359457, -0.00367774, -0.00355735, -0.00310642, -0.00242841],\n",
       "        [-0.42655374,  0.01946647,  0.01772517,  0.15672108, -0.02454069,\n",
       "          0.23357364,  0.0088998 , -0.09068006, -0.1466825 ,  1.06723062,\n",
       "         -0.02870436, -0.03745159, -0.03546536, -0.03256443, -0.03162106,\n",
       "         -0.00499147,  0.01830506,  0.02024084, -0.03497756, -0.03036203],\n",
       "        [-0.07849852,  0.54515472,  0.02864686, -0.07759887, -0.12747447,\n",
       "         -0.09361672,  0.0719206 , -0.01474264, -0.00402781, -0.02870436,\n",
       "          1.14857754,  1.14515904,  1.11930741,  1.13552123,  1.12056662,\n",
       "          1.12389544,  1.08557108,  1.0834504 ,  1.13844896,  1.09125126],\n",
       "        [-0.0625894 ,  0.5254915 ,  0.01319037, -0.08396534, -0.12205171,\n",
       "         -0.08394645,  0.09733441,  0.00247296, -0.00224139, -0.03745159,\n",
       "          1.14515904,  1.17060053,  1.12389578,  1.15969277,  1.14306164,\n",
       "          1.13738989,  1.06831236,  1.08058926,  1.1461099 ,  1.12997837],\n",
       "        [-0.10638233,  0.65648999,  0.10259754, -0.08410067, -0.14503987,\n",
       "         -0.09491994,  0.0846838 , -0.00958805, -0.00250723, -0.03546536,\n",
       "          1.11930741,  1.12389578,  1.15869087,  1.1084956 ,  1.08475769,\n",
       "          1.0705618 ,  1.02964426,  1.01047334,  1.10533315,  1.0142568 ],\n",
       "        [-0.06399604,  0.48385485,  0.10398167, -0.07247051, -0.12625888,\n",
       "         -0.08410296,  0.11169439, -0.01344733, -0.00263364, -0.03256443,\n",
       "          1.13552123,  1.15969277,  1.1084956 ,  1.16593782,  1.15993537,\n",
       "          1.14325012,  1.08208059,  1.10130463,  1.14186315,  1.13887761],\n",
       "        [-0.06722267,  0.44037878,  0.15931258, -0.06420984, -0.11951228,\n",
       "         -0.09002395,  0.12136506, -0.03076694, -0.00297991, -0.03162106,\n",
       "          1.12056662,  1.14306164,  1.08475769,  1.15993537,  1.16450611,\n",
       "          1.14057972,  1.079156  ,  1.10373927,  1.12921527,  1.14092616],\n",
       "        [-0.06021107,  0.44438449,  0.07760908, -0.06823602, -0.11724608,\n",
       "         -0.07850486,  0.12538785, -0.02994661, -0.00359457, -0.00499147,\n",
       "          1.12389544,  1.13738989,  1.0705618 ,  1.14325012,  1.14057972,\n",
       "          1.15569937,  1.11575526,  1.12536373,  1.13897525,  1.14070968],\n",
       "        [-0.07283895,  0.46293975,  0.11306478, -0.05809714, -0.12193292,\n",
       "         -0.06423026,  0.1137582 , -0.03623643, -0.00367774,  0.01830506,\n",
       "          1.08557108,  1.06831236,  1.02964426,  1.08208059,  1.079156  ,\n",
       "          1.11575526,  1.15023168,  1.13270752,  1.09989532,  1.05958794],\n",
       "        [-0.0572483 ,  0.3993541 ,  0.11240146, -0.05511243, -0.11291085,\n",
       "         -0.06081991,  0.11056804, -0.0260855 , -0.00355735,  0.02024084,\n",
       "          1.0834504 ,  1.08058926,  1.01047334,  1.10130463,  1.10373927,\n",
       "          1.12536373,  1.13270752,  1.14042135,  1.10202   ,  1.09882934],\n",
       "        [-0.07002818,  0.51153128,  0.04528774, -0.08118742, -0.11902332,\n",
       "         -0.09106937,  0.1053126 , -0.02150154, -0.00310642, -0.03497756,\n",
       "          1.13844896,  1.1461099 ,  1.10533315,  1.14186315,  1.12921527,\n",
       "          1.13897525,  1.09989532,  1.10202   ,  1.14883955,  1.11440995],\n",
       "        [-0.02651736,  0.34199703,  0.03214024, -0.07043258, -0.09996177,\n",
       "         -0.07574246,  0.13415453, -0.01481751, -0.00242841, -0.03036203,\n",
       "          1.09125126,  1.12997837,  1.0142568 ,  1.13887761,  1.14092616,\n",
       "          1.14070968,  1.05958794,  1.09882934,  1.11440995,  1.1734158 ]]),\n",
       " array([1.00479627, 1.01722352, 1.0216003 , 1.06363251, 1.00869793,\n",
       "        1.05963785, 1.05648853, 1.03118052, 1.08840511, 1.03306855,\n",
       "        1.0717171 , 1.08194294, 1.07642504, 1.07978601, 1.07912284,\n",
       "        1.07503459, 1.07248854, 1.06790512, 1.07183933, 1.08324318]))"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(store_covariance=True)\n",
    "lda.fit(X_train, y_train)\n",
    "lda.means_, lda.priors_, lda.covariance_, np.sqrt(np.diag(lda.covariance_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.74, 0.7)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(X_train, y_train).score(X_train, y_train), lda.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False False False False False False  True False False False\n",
      "  False False False False False False False False]\n",
      " [False False False False False False False False  True  True False False\n",
      "  False False False False False False False False]\n",
      " [False False False False False False False False  True  True  True False\n",
      "  False False False False False False False False]\n",
      " [False False False False False False False False  True  True  True  True\n",
      "  False False False False False False False False]\n",
      " [False False False False  True False False False  True  True  True  True\n",
      "  False False False False False False False False]\n",
      " [False False False False  True False False False  True  True  True  True\n",
      "   True False False False False False False False]\n",
      " [False False False False  True False False False  True  True  True  True\n",
      "   True False  True False False False False False]\n",
      " [False False False False  True False False False  True  True  True  True\n",
      "   True False  True  True False False False False]\n",
      " [False False False  True  True False False False  True  True  True  True\n",
      "   True False  True  True False False False False]\n",
      " [ True False False  True  True False False False  True  True  True  True\n",
      "   True False  True  True False False False False]\n",
      " [ True False False  True  True False False False  True  True  True  True\n",
      "   True False  True  True  True False False False]\n",
      " [ True False False  True  True False  True False  True  True  True  True\n",
      "   True False  True  True  True False False False]\n",
      " [ True  True False  True  True False  True False  True  True  True  True\n",
      "   True False  True  True  True False False False]\n",
      " [ True  True False  True  True False  True False  True  True  True  True\n",
      "   True False  True  True  True  True False False]\n",
      " [ True  True False  True  True False  True False  True  True  True  True\n",
      "   True  True  True  True  True  True False False]\n",
      " [ True  True False  True  True False  True False  True  True  True  True\n",
      "   True  True  True  True  True  True False  True]\n",
      " [ True  True False  True  True False  True False  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True False  True  True  True  True False  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True False  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "   True  True  True  True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# forward subset selection\n",
    "best_k_features = np.zeros((p,p), bool)\n",
    "vals_of_k = np.zeros((p), bool)\n",
    "for k in range(p):\n",
    "    best = 0.5\n",
    "    best_new =  -1\n",
    "    for l in [i for i, x in enumerate(~best_k_features[k,:]) if x]:\n",
    "        best_k_features[range(k,p),l] = True\n",
    "        new_score =  lda.fit(X_train[:,best_k_features[k,:]], y_train).score(X_train[:,best_k_features[k,:]], y_train)\n",
    "        if new_score > best:\n",
    "            best = new_score\n",
    "            if best_new != -1:\n",
    "                best_k_features[range(k,p),best_new] = False  \n",
    "            best_new = l\n",
    "        else:\n",
    "            best_k_features[range(k,p),l] = False\n",
    "print(best_k_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "# 5-fold CV for best models with k features\n",
    "k_cv =  5\n",
    "n_train = y_train.shape[0]\n",
    "best_k = 0\n",
    "best_cv_score = 0.5\n",
    "length_cv = int(n_train/k_cv)\n",
    "for k in range(1,p):\n",
    "    cv_score = 0\n",
    "    selected_features = best_k_features[k,:]\n",
    "    for n_cv in range(k_cv):\n",
    "        indices_cv = np.zeros(n_train)!=0\n",
    "        indices_cv[np.full((length_cv),length_cv*n_cv)+range(length_cv)]=True \n",
    "        y_train_cv,y_test_cv = y_train[indices_cv],y_train[~indices_cv]\n",
    "        X_train_cv,X_test_cv = X_train[indices_cv,:][:,selected_features], X_train[~indices_cv,:][:,selected_features]\n",
    "        cv_score += lda.fit(X_train_cv, y_train_cv).score(X_test_cv, y_test_cv)\n",
    "    cv_score /= k_cv\n",
    "    if cv_score > best_cv_score:\n",
    "        best_cv_score = cv_score\n",
    "        best_k = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 0.6)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k, best_cv_score # best cv feature number and validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_in = best_k_features[best_k,:]\n",
    "# take_in = (True, True, False, False, False, False, False, False, True, False, False, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False,  True, False, False, False,  True,\n",
       "        True,  True,  True,  True, False,  True,  True, False, False,\n",
       "       False, False])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training error for best cv feature number \n",
    "lda.fit(X_train[:,take_in], y_train).score(X_train[:,take_in], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test error for best cv feature number \n",
    "lda.fit(X_train[:,best_k_features[best_k,:]], y_train).score(X_test[:,best_k_features[best_k,:]], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-ff3993bb52fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbest_k_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_k_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mnew_score\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     def fit(self, start_params=None, method='newton', maxiter=35,\n\u001b[1;32m   1831\u001b[0m             full_output=1, disp=1, callback=None, **kwargs):\n\u001b[0;32m-> 1832\u001b[0;31m         bnryfit = super(Logit, self).fit(start_params=start_params,\n\u001b[0m\u001b[1;32m   1833\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m                 disp=disp, callback=callback, **kwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# forward subset selection\n",
    "best_k_features = np.zeros((p,p), bool)\n",
    "vals_of_k = np.zeros((p), bool)\n",
    "for k in range(p):\n",
    "    print(k)\n",
    "    best = 0.5\n",
    "    best_new =  -1\n",
    "    for l in [i for i, x in enumerate(~best_k_features[k,:]) if x]:\n",
    "        best_k_features[range(k,p),l] = True\n",
    "        Xt = sm.add_constant(X_train[:,best_k_features[k,:]])\n",
    "        new_score =  sm.Logit.fit(Xt, y_train).score(Xt, y_train)\n",
    "        if new_score > best:\n",
    "            best = new_score\n",
    "            if best_new != -1:\n",
    "                best_k_features[range(k,p),best_new] = False  \n",
    "            best_new = l\n",
    "        else:\n",
    "            best_k_features[range(k,p),l] = False\n",
    "print(best_k_features)\n",
    "\n",
    "#model = sm.Logit(yt, Xt).fit()\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-fold CV for best models with k features\n",
    "k_cv =  6\n",
    "n_train = y_train.shape[0]\n",
    "best_k = 0\n",
    "best_cv_score = 0.3\n",
    "length_cv = int(n_train/k_cv)\n",
    "for k in range(1,p):\n",
    "    cv_score = 0\n",
    "    selected_features = best_k_features[k,:]\n",
    "    for n_cv in range(k_cv):\n",
    "        indices_cv = np.zeros(n_train)!=0\n",
    "        indices_cv[np.full((length_cv),length_cv*n_cv)+range(length_cv)]=True \n",
    "        y_train_cv,y_test_cv = y_train[indices_cv],y_train[~indices_cv]\n",
    "        X_train_cv,X_test_cv = X_train[indices_cv,:], X_train[~indices_cv,:]\n",
    "        X_train_cv,X_test_cv = sm.add_constant(X_train_cv[:,selected_features]), sm.add_constant(X_test_cv[:,selected_features])     \n",
    "        cv_score += sm.Logit.fit(X_train_cv, y_train_cv).score(X_test_cv, y_test_cv)\n",
    "    cv_score /= k_cv\n",
    "    if cv_score > best_cv_score:\n",
    "        best_cv_score = cv_score\n",
    "        best_k = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_post(beta, x, y, m0, S0):\n",
    "    logprior =  - .5* (beta-m0).T @ S0 @ (beta-m0)\n",
    "    return  nll(beta, x, y) -  logprior\n",
    "\n",
    "n_train,p =X_train.shape\n",
    "m0 = np.zeros(p)\n",
    "S0 = X_train.T.dot(X_train)/n_train\n",
    "neg_post(beta_mle, X_train, y_train, m0, S0)\n",
    "\n",
    "import scipy.optimize as opt\n",
    "\n",
    "beta0 = np.ones(p) * .1\n",
    "myoptions={'disp':True}\n",
    "results = opt.minimize(neg_post,beta0,args=(X_train, y_train, m0, S0),\n",
    "options = myoptions)\n",
    "beta_map = results.x\n",
    "print(beta_map)\n",
    "\n",
    "def laplace_prec(beta, x, S0):\n",
    "    n = x.shape[0]  \n",
    "    S = np.diag(sigma(x,beta)*(1-sigma(x,beta)))\n",
    "    prec = S0.values + x.T.dot(S.dot(x))    \n",
    "    return prec\n",
    "\n",
    "S0 = X_train.T.dot(X_train)/n_train #linear regression unit information prior\n",
    "#get posterior covariance evaluating at beta_map \n",
    "prec = laplace_prec(beta_map, X_train, S0)\n",
    "cov = sc.linalg.inv(prec)\n",
    "\n",
    "#95% credible intervals\n",
    "se = np.sqrt(np.diag(cov))\n",
    "lower95 = beta_map - 1.96*se\n",
    "upper95 = beta_map + 1.96*se\n",
    "\n",
    "#present the output via a pandas data frame\n",
    "results = np.column_stack([beta_map,se,lower95,upper95])\n",
    "col = ['post mean','post se','lower 95% bound','upper 95% bound']\n",
    "ind = ['intercept','age','nodes_detected']\n",
    "results = pd.DataFrame(results,columns = col,index=ind)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
